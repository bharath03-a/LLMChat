{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Union\n",
    "from IPython.display import Image, display\n",
    "import weaviate\n",
    "from weaviate.auth import Auth\n",
    "import weaviate.classes as wvc\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Importing necessary libraries\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from tavily import TavilyClient\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Process legal documents and create vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, documents_dir: str = \"./notes\"):\n",
    "        self.documents_dir = documents_dir\n",
    "        self.weaviate_url = os.environ.get(\"WEAVIATE_URL\")\n",
    "        self.weaviate_api_key = os.environ.get(\"WEAVIATE_API_KEY\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "    def load_documents(self) -> List[Any]:\n",
    "        \"\"\"Load documents from the directory\"\"\"\n",
    "        try:\n",
    "            loader = DirectoryLoader(\n",
    "                self.documents_dir,\n",
    "                glob=\"**/*.pdf\",\n",
    "                loader_cls=PyPDFLoader\n",
    "            )\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} documents.\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_documents(self) -> List[Any]:\n",
    "        \"\"\"Split documents into chunks\"\"\"\n",
    "        documents = self.load_documents()\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Split into {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def create_vector_store(self) -> WeaviateVectorStore:\n",
    "        \"\"\"Create and populate vector store with documents using LangChain's WeaviateVectorStore\"\"\"\n",
    "        \n",
    "        client = weaviate.connect_to_weaviate_cloud(\n",
    "            cluster_url=self.weaviate_url,\n",
    "            auth_credentials=Auth.api_key(self.weaviate_api_key),\n",
    "        )\n",
    "        \n",
    "        chunks = self.process_documents()\n",
    "        \n",
    "        if client.collections.exists(\"LegalDocuments\"):\n",
    "            client.collections.delete(\"LegalDocuments\")\n",
    "        \n",
    "        vector_store = WeaviateVectorStore.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            client=client,\n",
    "            index_name=\"LegalDocuments\", \n",
    "            text_key=\"content\",\n",
    "            by_text=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully imported {len(chunks)} chunks into Weaviate\")\n",
    "        return vector_store\n",
    "    \n",
    "    def query_store(self, query: str, vector_store: WeaviateVectorStore, k: int = 5):\n",
    "        \"\"\"Query the vector store for similar documents\"\"\"\n",
    "        docs = vector_store.similarity_search(query, k=k)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1465 documents.\n",
      "Split into 5892 chunks\n",
      "Successfully imported 5892 chunks into Weaviate\n",
      "\n",
      "Document 1:\n",
      "Source: ../streamlit_app/notes/introduction-to-law.pdf, Page: 380.0\n",
      "11\n",
      "The development and role of the\n",
      "contract\n",
      "The contract is the legal cornerstone of all transactions in business and consumer\n",
      "life. It is the legal d...\n",
      "\n",
      "Document 2:\n",
      "Source: ../streamlit_app/notes/introduction-to-law.pdf, Page: 409.0\n",
      "Finally, there is in the 1977 Act a wide power enabling courts to declare invalid any\n",
      "clause in standard form contracts where a trader attempts to exc...\n",
      "\n",
      "Document 3:\n",
      "Source: ../streamlit_app/notes/introduction-to-law.pdf, Page: 400.0\n",
      "64 Sale and Supply of Goods Act 1994, discussed below. 65 Andrews v Singer, op. cit.\n",
      "66 For example, Karsales (Harrow) Ltd v Wallis [1956] 1 WLR 936; ...\n",
      "\n",
      "Document 4:\n",
      "Source: ../streamlit_app/notes/introduction-to-law.pdf, Page: 409.0\n",
      "to terminate the contract, whilst breach of a warranty entitles the aggrieved party to sue for\n",
      "damages only. Whether a given contractual term is a con...\n",
      "\n",
      "Document 5:\n",
      "Source: ../streamlit_app/notes/international_law_handbook.pdf, Page: 64.0\n",
      "tion of the treaty in whole or in part in the relations between itself and the defaulting State;\n",
      "(c) any party other than the defaulting State to invo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_Assistant/lib/python3.11/site-packages/weaviate/warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = DocumentProcessor(documents_dir=\"../streamlit_app/notes/\")\n",
    "vector_store = processor.create_vector_store()\n",
    "\n",
    "results = processor.query_store(\"contract breach provisions\", vector_store)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "    print(doc.page_content[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedAgentState(TypedDict):\n",
    "    \"\"\"Enhanced state management for the Legal AI Assistant\"\"\"\n",
    "    input: Any  # Raw input (text, image, PDF)\n",
    "    input_type: str  # Type of input (text, image, PDF)\n",
    "    processed_input: Optional[Dict[str, Any]]  # Processed input data\n",
    "    query_details: Optional[Dict[str, Any]]  # Decomposed query\n",
    "    document_search_results: Optional[List[Dict[str, Any]]]  # Results from document search\n",
    "    document_search_sufficient: Optional[bool]  # Whether document search results are sufficient\n",
    "    web_search_results: Optional[List[Dict[str, Any]]]  # Results from web search\n",
    "    web_search_sufficient: Optional[bool]  # Whether web search results are sufficient\n",
    "    need_additional_search: Optional[bool]  # Whether additional search is needed\n",
    "    final_response: Optional[str]  # Final response content\n",
    "    references: Optional[List[str]]  # References\n",
    "    conversation_history: List[Union[HumanMessage, AIMessage]]  # Conversation history\n",
    "\n",
    "def determine_search_sufficiency(state: EnhancedAgentState, search_type: str, threshold: float = 7.0) -> Dict[str, Any]:\n",
    "    \"\"\"Determine if search results are sufficient based on relevance score\"\"\"\n",
    "    if search_type == \"document\":\n",
    "        evaluation = state.get(\"document_search_evaluation\", {})\n",
    "        relevance_score = evaluation.get(\"Relevance Score\", 0)\n",
    "        sufficient = relevance_score >= threshold\n",
    "        \n",
    "        return {\n",
    "            \"document_search_sufficient\": sufficient,\n",
    "            \"need_additional_search\": not sufficient\n",
    "        }\n",
    "    elif search_type == \"web\":\n",
    "        evaluation = state.get(\"web_search_evaluation\", {})\n",
    "        relevance_score = evaluation.get(\"Relevance Score\", 0)\n",
    "        sufficient = relevance_score >= threshold\n",
    "        \n",
    "        return {\n",
    "            \"web_search_sufficient\": sufficient,\n",
    "            \"need_additional_search\": state.get(\"need_additional_search\", True) and not sufficient\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported search type: {search_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up AI Agent workflow\n",
    "class LegalAIAssistant:\n",
    "    def __init__(self, weaviate_url: Optional[str] = None):\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            temperature=0.6,\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        self.tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    \n",
    "        self.document_processor = DocumentProcessor(documents_dir=\"../streamlit_app/notes/\")\n",
    "        self.vector_store = self.document_processor.create_vector_store()\n",
    "        \n",
    "        self.query_understanding_system = \"\"\"You are an expert legal AI assistant specializing in understanding complex legal queries.\n",
    "        Your task is to analyze the user's input and break it down into components that will guide a comprehensive legal search and response.\n",
    "        Pay special attention to:\n",
    "        1. Identifying the core legal issue or question\n",
    "        2. Determining relevant jurisdictions\n",
    "        3. Identifying specific legal domains (criminal, civil, corporate, etc.)\n",
    "        4. Extracting potential subqueries that need separate investigation\n",
    "        5. Identifying any time-sensitive elements\n",
    "        \n",
    "        Format your analysis as a structured JSON object.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.document_evaluation_system = \"\"\"You are an expert legal document analyst.\n",
    "        Your task is to evaluate search results from a legal document database and determine if they adequately address the user's query.\n",
    "        Consider:\n",
    "        1. Relevance of the documents to the specific legal question\n",
    "        2. Comprehensiveness of the information provided\n",
    "        3. Accuracy and authority of the sources\n",
    "        4. Whether the information is complete or requires additional context\n",
    "        \n",
    "        Assign a relevance score (0-10) and explain your reasoning.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.web_evaluation_system = \"\"\"You are an expert legal research analyst.\n",
    "        Your task is to evaluate web search results and determine if they adequately address aspects of the user's legal query.\n",
    "        Consider:\n",
    "        1. Credibility of the sources (government sites, law firms, legal journals)\n",
    "        2. Relevance to the specific legal question\n",
    "        3. Currency of the information (especially important for evolving legal topics)\n",
    "        4. Whether the results complement the document search results\n",
    "        \n",
    "        Assign a relevance score (0-10) and explain your reasoning.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.final_response_system = \"\"\"You are a comprehensive legal AI assistant tasked with providing accurate, nuanced, and helpful legal information.\n",
    "        When generating your response:\n",
    "        1. Focus on factual legal information and procedural guidance\n",
    "        2. Clearly distinguish between established law, legal interpretation, and practical advice\n",
    "        3. Include relevant citations and references to legal statutes, cases, or authorities\n",
    "        4. Provide balanced perspectives where legal interpretations differ\n",
    "        5. Clarify any jurisdictional limitations to your advice\n",
    "        6. Include appropriate disclaimers about not providing legal advice\n",
    "        \n",
    "        Structure your response in a clear, logical format with headings where appropriate.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize prompts\n",
    "        self._initialize_prompts()\n",
    "\n",
    "        self.visualize_workflow(\"./img/workflow.png\")\n",
    "    \n",
    "    def _initialize_prompts(self):\n",
    "        \"\"\"Initialize all prompts used by the assistant\"\"\"\n",
    "        # Query Understanding Prompt\n",
    "        self.query_understanding_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.query_understanding_system),\n",
    "            (\"human\", \"\"\"Analyze the following legal query and break it down into its key components:\n",
    "\n",
    "            {processed_input}\n",
    "\n",
    "            Return a structured JSON with these fields:\n",
    "            - core_legal_issue: The main legal question or problem\n",
    "            - jurisdiction: Relevant legal jurisdiction(s) if specified or can be inferred\n",
    "            - legal_domains: List of relevant legal areas (e.g., criminal, civil, property)\n",
    "            - subqueries: List of related questions that might need separate investigation\n",
    "            - time_sensitivity: Any urgent aspects of the query\n",
    "            - key_terms: Important legal terms mentioned or implied in the query\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        # Document Search Evaluation Prompt\n",
    "        self.document_evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.document_evaluation_system),\n",
    "            (\"human\", \"\"\"Evaluate these document search results for the legal query:\n",
    "\n",
    "            Query Details: {query_details}\n",
    "\n",
    "            Document Search Results:\n",
    "            {document_search_results}\n",
    "\n",
    "            Provide a JSON response with these fields:\n",
    "            - Relevance Score: (0-10)\n",
    "            - Key Matching Sections: List of sections most relevant to the query\n",
    "            - Information Gaps: Legal aspects of the query not covered by these documents\n",
    "            - Confidence Assessment: Your confidence in the documents answering the query correctly\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        # Web Search Evaluation Prompt\n",
    "        self.web_evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.web_evaluation_system),\n",
    "            (\"human\", \"\"\"Evaluate these web search results for the legal query:\n",
    "\n",
    "            Query Details: {query_details}\n",
    "\n",
    "            Web Search Results:\n",
    "            {web_search_results}\n",
    "\n",
    "            Provide a JSON response with these fields:\n",
    "            - Relevance Score: (0-10)\n",
    "            - Key Insights: Main legal information found in the results\n",
    "            - Source Credibility: Assessment of the credibility of the sources\n",
    "            - Information Gaps: Aspects of the query not adequately addressed\n",
    "            - Comparison to Document Results: How these results complement the document search\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        # Final Response Generation Prompt\n",
    "        self.final_response_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.final_response_system),\n",
    "            (\"human\", \"\"\"Generate a comprehensive legal response based on the following:\n",
    "\n",
    "            Original Query: {processed_input}\n",
    "            Query Analysis: {query_details}\n",
    "            Document Search Results: {document_search_results}\n",
    "            Web Search Results: {web_search_results}\n",
    "\n",
    "            Your response should include:\n",
    "            1. A clear explanation of the legal concepts and principles\n",
    "            2. Applicable laws, regulations, or precedents\n",
    "            3. Practical guidance on how to proceed\n",
    "            4. Any necessary disclaimers about jurisdictional limitations\n",
    "            5. References to sources used\n",
    "\n",
    "            Remember to remain balanced, factual, and helpful while acknowledging legal complexities.\n",
    "            \"\"\")\n",
    "        ])\n",
    "    \n",
    "    def process_input_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Process the input based on its type\"\"\"\n",
    "        processed_input = self.input_handler.process_input(\n",
    "            state['input'], \n",
    "            state['input_type']\n",
    "        )\n",
    "        \n",
    "        return {\"processed_input\": processed_input}\n",
    "    \n",
    "    def understand_query_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for understanding the query\"\"\"\n",
    "        chain = self.query_understanding_prompt | self.llm | JsonOutputParser()\n",
    "        query_details = chain.invoke({\"processed_input\": state['processed_input']['content']})\n",
    "        \n",
    "        # Add the query to conversation history\n",
    "        conversation_history = state.get('conversation_history', [])\n",
    "        conversation_history.append(HumanMessage(content=state['processed_input']['content']))\n",
    "        \n",
    "        return {\n",
    "            \"query_details\": query_details,\n",
    "            \"conversation_history\": conversation_history\n",
    "        }\n",
    "\n",
    "    def document_search_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for searching legal documents\"\"\"\n",
    "        # Extract key terms from query details\n",
    "        key_terms = state['query_details'].get('key_terms', [])\n",
    "        core_issue = state['query_details'].get('core_legal_issue', '')\n",
    "        \n",
    "        # Combine terms for search\n",
    "        search_query = f\"{core_issue} {' '.join(key_terms)}\"\n",
    "        \n",
    "        # Use vector store to search documents\n",
    "        search_results = self.vector_store.similarity_search_with_score(\n",
    "            query=search_query,\n",
    "            k=5,\n",
    "        )\n",
    "        \n",
    "        # Format results for the LLM\n",
    "        document_search_results = [\n",
    "            {\n",
    "                \"source\": result[0].metadata.get('source', 'Unknown'),\n",
    "                \"page\": result[0].metadata.get('page', 0),\n",
    "                \"relevance_score\": result[1],\n",
    "                \"content\": result[0].page_content\n",
    "            }\n",
    "            for result in search_results\n",
    "        ]\n",
    "        \n",
    "        # Evaluate search results\n",
    "        chain = self.document_evaluation_prompt | self.llm | JsonOutputParser()\n",
    "        document_evaluation = chain.invoke({\n",
    "            \"query_details\": state['query_details'],\n",
    "            \"document_search_results\": document_search_results\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"document_search_results\": document_search_results,\n",
    "            \"document_search_evaluation\": document_evaluation\n",
    "        }\n",
    "\n",
    "    def evaluate_doc_search_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for evaluating document search results and deciding next steps\"\"\"\n",
    "        return determine_search_sufficiency(state, \"document\")\n",
    "\n",
    "    def web_search_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for web searching\"\"\"\n",
    "        # Use the core legal issue and key terms for search\n",
    "        core_issue = state['query_details'].get('core_legal_issue', '')\n",
    "        jurisdiction = state['query_details'].get('jurisdiction', '')\n",
    "        \n",
    "        # Construct a more specific query for web search\n",
    "        web_query = f\"{core_issue} legal {jurisdiction}\"\n",
    "        \n",
    "        web_search_results = self.tavily_client.search(\n",
    "            query=web_query, \n",
    "            max_results=5,\n",
    "            search_depth=\"advanced\"\n",
    "        )\n",
    "        \n",
    "        # Evaluate web search results\n",
    "        chain = self.web_evaluation_prompt | self.llm | JsonOutputParser()\n",
    "        web_search_evaluation = chain.invoke({\n",
    "            \"query_details\": state['query_details'],\n",
    "            \"web_search_results\": web_search_results['results']\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"web_search_results\": web_search_results['results'],\n",
    "            \"web_search_evaluation\": web_search_evaluation\n",
    "        }\n",
    "\n",
    "    def evaluate_web_search_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for evaluating web search results and deciding next steps\"\"\"\n",
    "        return determine_search_sufficiency(state, \"web\")\n",
    "\n",
    "    def generate_final_response_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for generating final comprehensive response\"\"\"\n",
    "        chain = self.final_response_prompt | self.llm\n",
    "        final_response = chain.invoke({\n",
    "            \"processed_input\": state['processed_input']['content'],\n",
    "            \"query_details\": state['query_details'],\n",
    "            \"document_search_results\": state['document_search_results'],\n",
    "            \"web_search_results\": state['web_search_results']\n",
    "        })\n",
    "        \n",
    "        # Collect references\n",
    "        references = []\n",
    "        \n",
    "        # Add document references\n",
    "        for doc in state.get('document_search_results', []):\n",
    "            source = doc.get('source', '')\n",
    "            page = doc.get('page', '')\n",
    "            if source and source not in references:\n",
    "                references.append(f\"{source} (Page {page})\")\n",
    "        \n",
    "        # Add web references\n",
    "        for result in state.get('web_search_results', []):\n",
    "            url = result.get('url', '')\n",
    "            if url and url not in references:\n",
    "                references.append(url)\n",
    "        \n",
    "        # Add to conversation history\n",
    "        conversation_history = state.get('conversation_history', [])\n",
    "        conversation_history.append(AIMessage(content=final_response.content))\n",
    "        \n",
    "        return {\n",
    "            \"final_response\": final_response.content,\n",
    "            \"references\": references,\n",
    "            \"conversation_history\": conversation_history\n",
    "        }\n",
    "    \n",
    "    def additional_search_node(self, state: EnhancedAgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Node for performing additional searches when needed\"\"\"\n",
    "        # Identify information gaps from evaluations\n",
    "        doc_eval = state.get('document_search_evaluation', {})\n",
    "        web_eval = state.get('web_search_evaluation', {})\n",
    "        \n",
    "        info_gaps_doc = doc_eval.get('Information Gaps', [])\n",
    "        info_gaps_web = web_eval.get('Information Gaps', [])\n",
    "        \n",
    "        # Combine information gaps\n",
    "        all_gaps = info_gaps_doc + info_gaps_web\n",
    "        \n",
    "        # Use Tavily for specialized search on the gaps\n",
    "        additional_results = []\n",
    "        for gap in all_gaps:\n",
    "            if isinstance(gap, str) and gap.strip():\n",
    "                try:\n",
    "                    gap_results = self.tavily_client.search(\n",
    "                        query=f\"{gap} legal information {state['query_details'].get('jurisdiction', '')}\",\n",
    "                        max_results=2,\n",
    "                        search_depth=\"advanced\"\n",
    "                    )\n",
    "                    additional_results.extend(gap_results['results'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in additional search: {e}\")\n",
    "        \n",
    "        # Combine with existing web search results\n",
    "        current_web_results = state.get('web_search_results', [])\n",
    "        combined_results = current_web_results + additional_results\n",
    "        \n",
    "        # Remove duplicates by URL\n",
    "        seen_urls = set()\n",
    "        unique_results = []\n",
    "        for result in combined_results:\n",
    "            url = result.get('url', '')\n",
    "            if url and url not in seen_urls:\n",
    "                seen_urls.add(url)\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        return {\n",
    "            \"web_search_results\": unique_results[:8],  # Limit to top 8 results\n",
    "            \"need_additional_search\": False  # Reset flag\n",
    "        }\n",
    "    \n",
    "    def should_perform_additional_search(self, state: EnhancedAgentState) -> str:\n",
    "        \"\"\"Decision node to determine if additional search is needed\"\"\"\n",
    "        if state.get(\"need_additional_search\", False):\n",
    "            return \"additional_search\"\n",
    "        return \"generate_response\"\n",
    "    \n",
    "    def build_workflow(self):\n",
    "        \"\"\"Construct the agentic workflow using LangGraph with decision points\"\"\"\n",
    "        workflow = StateGraph(EnhancedAgentState)\n",
    "        \n",
    "        # Add all nodes\n",
    "        workflow.add_node(\"process_input\", self.process_input_node)\n",
    "        workflow.add_node(\"understand_query\", self.understand_query_node)\n",
    "        workflow.add_node(\"document_search\", self.document_search_node)\n",
    "        workflow.add_node(\"evaluate_doc_search\", self.evaluate_doc_search_node)\n",
    "        workflow.add_node(\"web_search\", self.web_search_node)\n",
    "        workflow.add_node(\"evaluate_web_search\", self.evaluate_web_search_node)\n",
    "        workflow.add_node(\"additional_search\", self.additional_search_node)\n",
    "        workflow.add_node(\"generate_response\", self.generate_final_response_node)\n",
    "        \n",
    "        # Define workflow edges with decision points\n",
    "        workflow.set_entry_point(\"process_input\")\n",
    "        workflow.add_edge(\"process_input\", \"understand_query\")\n",
    "        workflow.add_edge(\"understand_query\", \"document_search\")\n",
    "        workflow.add_edge(\"document_search\", \"evaluate_doc_search\")\n",
    "        \n",
    "        # Decision after document search\n",
    "        workflow.add_conditional_edges(\n",
    "            \"evaluate_doc_search\",\n",
    "            self.should_perform_additional_search,\n",
    "            {\n",
    "                \"additional_search\": \"additional_search\",\n",
    "                \"generate_response\": \"web_search\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"web_search\", \"evaluate_web_search\")\n",
    "        \n",
    "        # Decision after web search\n",
    "        workflow.add_conditional_edges(\n",
    "            \"evaluate_web_search\",\n",
    "            self.should_perform_additional_search,\n",
    "            {\n",
    "                \"additional_search\": \"additional_search\",\n",
    "                \"generate_response\": \"generate_response\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"additional_search\", \"generate_response\")\n",
    "        workflow.set_finish_point(\"generate_response\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def visualize_workflow(self, graph):\n",
    "        \"\"\"Visualize the LangGraph workflow with decision points and save it to a file.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            img = Image(graph.get_graph().draw_mermaid_png())\n",
    "            with open(\"mermaid_graph.png\", \"wb\") as f:\n",
    "                f.write(img.data)\n",
    "            print(\"Image saved as mermaid_graph.png\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    async def process_query(self, query: Any, input_type: str = \"text\"):\n",
    "        \"\"\"Async method to process user query with any input type\"\"\"\n",
    "        workflow = self.build_workflow()\n",
    "        initial_state = {\n",
    "            \"input\": query,\n",
    "            \"input_type\": input_type,\n",
    "            \"conversation_history\": []\n",
    "        }\n",
    "        \n",
    "        result = await workflow.ainvoke(initial_state)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1465 documents.\n",
      "Split into 5892 chunks\n",
      "Successfully imported 5892 chunks into Weaviate\n",
      "Error: 'str' object has no attribute 'get_graph'\n"
     ]
    }
   ],
   "source": [
    "assistant = LegalAIAssistant()\n",
    "query = \"What are the legal implications of breaking a non-compete agreement in California?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = assistant.build_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as mermaid_graph.png\n"
     ]
    }
   ],
   "source": [
    "assistant.visualize_workflow(workflow_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
